# -*- coding: utf-8 -*-
"""Emotion Regression VGGFace.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ogGD5kRPKsAC-2L7XK1pYati8MFLbs1v
"""

print("Bismillah")

"""# **Imports**"""

# Commented out IPython magic to ensure Python compatibility.
!pip install ipython-autotime
# %load_ext autotime

# Commented out IPython magic to ensure Python compatibility.
# necessary imports
import os
import cv2
import numpy as np
from imutils import paths
from sklearn.preprocessing import LabelBinarizer
from tqdm import tqdm

import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab.patches import cv2_imshow

"""# Initializing"""

img_height = 200
img_width = 200
batch_size = 32
nb_epochs = batch_size
patience_val = 10
model_name = "VGGFace_15000_Onwards_1"
cutout_size = 20

"""# VGG FACE"""

!sudo pip install git+https://github.com/rcmalli/keras-vggface.git

!pip install --upgrade keras_applications
from keras_vggface.vggface import VGGFace
from keras.models import Model
from keras.layers import Dense

model = VGGFace(model='senet50', include_top=False, input_shape=(img_width, img_height, 3), pooling='avg')

class1 = Dense(1024, activation='relu')(model.layers[-1].output) # add FC layer on last layer
class2 = Dense(512, activation='relu')(class1) # add FC layer on previous layer
output = Dense(2)(class2) # add wx+b
model = Model(inputs=model.inputs, outputs=output)
model.summary()

"""# Compile the model"""

model.compile(
    optimizer='adam',
    loss='mean_squared_error',
    metrics='MeanSquaredError')

"""# On the Fly Data Load"""

from keras.preprocessing.image import ImageDataGenerator
import pandas as pd
Trainingdf = pd.read_csv("/content/drive/MyDrive/CV/cv project/CV-Project/Aff-Wild/Training.csv")

training = Trainingdf.iloc[30000:45000]

# from sklearn.utils import shuffle
# training = shuffle(training)

"""Loading Training Images"""

train_datagen = ImageDataGenerator(
    rescale=1./255
    ,validation_split=0.2) # set validation split

train_generator = train_datagen.flow_from_dataframe(
    training,
    directory=None,
    x_col="Image_Path",
    y_col=["Valence","Arousal"],
    target_size=(img_width, img_height),
    color_mode="rgb",
    class_mode="raw",
    batch_size=batch_size,
    shuffle=True,
    seed=11,
    subset="training"
    # interpolation="nearest",
)

"""Loading Validation images"""

validation_generator = train_datagen.flow_from_dataframe(
    training,
    directory=None,
    x_col="Image_Path",
    y_col=["Valence","Arousal"],
    target_size=(img_width, img_height),
    color_mode="rgb",
    class_mode="raw",
    batch_size=batch_size,
    shuffle=True,
    seed=11,
    subset="validation"
    # interpolation="nearest",
)

"""# **Save Path for the outputs**"""

save_path = '/content/drive/MyDrive/CV/cv project/CV-Project/Aff-Wild/TransferLearningOutputs/'+model_name

"""# Train the model"""

from keras.callbacks import ModelCheckpoint, EarlyStopping

# patient early stopping 
es = EarlyStopping(monitor='mean_squared_error', mode='min', verbose=1, patience=patience_val)
mc = ModelCheckpoint(save_path+'/best_model.h5', monitor='val_mean_squared_error', mode='min', verbose=1, save_best_only=True)

H = model.fit(
    train_generator,
    steps_per_epoch = train_generator.samples // batch_size,
    validation_data = validation_generator, 
    validation_steps = validation_generator.samples // batch_size,
    epochs = nb_epochs
    , callbacks= [es , mc]
    )

"""# Saving Model Weights and History

Saving Model weights
"""

# save the model's trained weights
model.save_weights(save_path+"/_transferOnwards_trained_wts.h5")

model.load_weights(save_path+'/best_model.h5')

"""Saving History"""

import pickle
with open(save_path+'/historyOnwards', 'wb') as file_pi:
        pickle.dump(H.history, file_pi)

"""Loading history"""

import pickle
dbfile = open(save_path+'/historyOnwards', 'rb')      
db = pickle.load(dbfile) 
for keys in db: 
    print(keys, '=>', db[keys]) 
dbfile.close()

"""# Some Graphs"""

db["mean_squared_error"]

# simple_acc = H.history['mean_squared_error']

simple_acc = db['mean_squared_error']
plt.plot([acc for acc in simple_acc])
plt.title('Train Accuracy for '+ model_name)
plt.ylabel('Mean Squared Error on Training')
plt.xlabel('Epoch')
plt.savefig(save_path+'/Train_Acc.png')
plt.show()

# simple_val_acc = H.history['val_mean_squared_error']
simple_val_acc = db['val_mean_squared_error']
plt.plot([acc for acc in simple_val_acc])
plt.title('Validation Accuracy for'+ model_name)
plt.ylabel('Validation Mean squared error')
plt.xlabel('Epoch')
plt.savefig(save_path+'/Validation_Acc.png')
plt.show()

# simple_loss = H.history['loss']
simple_loss = db['loss']
plt.plot([los for los in simple_loss])
plt.title('Train Loss for '+ model_name)
plt.ylabel('Train Loss')
plt.xlabel('Epoch')
plt.savefig(save_path+'/Train_loss.png')
plt.show()

simple_val_loss = H.history['val_loss']
plt.plot([los for los in simple_val_loss])
plt.title('Validation Loss for '+ model_name)
plt.ylabel('Validation Loss')
plt.xlabel('Epoch')
plt.savefig(save_path+'/Validation_Loss.png')
plt.show()

"""Loss/Error Graphs"""

fig, axis = plt.subplots(1, 2, figsize=(20, 4))


axis[0].plot(db['mean_squared_error'],
         label='Train mean squared error',
         c='tomato', ls='-')
axis[0].plot(db['val_mean_squared_error'],
         label='Validation mean squared error',
         c='magenta', ls='-')

axis[0].set_xlabel('Epoch')
axis[0].set_ylabel('Accuracy')
axis[0].legend(loc='upper left')


axis[1].plot(db['loss'],
         label='Train loss',
         c='tomato', ls='-')
axis[1].plot(db['val_loss'],
         label='Validation loss',
         c='magenta', ls='-')

axis[1].set_xlabel('Epoch')
axis[1].set_ylabel('loss')
axis[1].legend(loc='upper left')
plt.savefig(save_path+'/simple_Validation_error&loss.png')
plt.show()

"""# Testing

On the fly load
"""

import pandas as pd
from keras.preprocessing.image import ImageDataGenerator

Testingdf = pd.read_csv("/content/drive/MyDrive/CV/cv project/CV-Project/Aff-Wild/Testing.csv")
testing = Testingdf.tail(5000)
test_datagen = ImageDataGenerator(rescale=1. / 255)

test_generator = test_datagen.flow_from_dataframe(
    testing,
    directory=None,
    x_col="Image_Path",
    y_col=["Valence","Arousal"],
    target_size=(img_width, img_height),
    color_mode="rgb",
    shuffle = False,
    class_mode="raw"
)

"""Testing Model"""

score = model.evaluate(test_generator, batch_size=batch_size)

import pickle
with open(save_path+'/score', 'wb') as file_pi:
        pickle.dump(score, file_pi)

print('Test Mean Squared Error = ', score[0])
print('Test Accuracy = ', score[1])

# GET Concordance Correlation Coefficient
prediction = model.predict(test_generator)

with open(save_path+'/predictions', 'wb') as file_pi:
        pickle.dump(prediction, file_pi)

import pickle
dbfile = open(save_path+'/predictions', 'rb')      
prediction = pickle.load(dbfile)

len(prediction)

prediction

prediction_v = prediction[10,0]
prediction_a = prediction[10,1]

plt.scatter(prediction[:,0],prediction[:,1])

plt.scatter(test_generator.labels[:,0],test_generator.labels[:,1])

print(prediction[10,0],prediction[10,1])

print(prediction_v, prediction_a)

if prediction_v >= -0.5 and prediction_v <= 0.5 and prediction_a <= -0.66:
    print("focus")
elif prediction_v >= -0.2 and prediction_a >= -0.66 and prediction_a <= 0:
    print("relaxation")
elif prediction_v <= 0 and prediction_a <= 0:
    print("stress")
elif prediction_v >= 0 and prediction_v <= 0.4 and prediction_a >= 0 and prediction_a <= 0.4:
    print("interest")
elif (prediction_v >= 0 and prediction_v <= 0.7 and prediction_a >= 0.4 and prediction_a <= 0.7 or
      prediction_v >= 0.4 and prediction_v <= 0.7 and prediction_a >= 0 and prediction_a <= 0.4 or
      prediction_v >= 0.7 and prediction_v <= 1 and prediction_a >= 0 and prediction_a <= 1):
    print("engagement")
elif prediction_v >= 0 and prediction_v <= 1 and prediction_a >= 0.7 and prediction_a <= 1:
    print("excitement")
else:
    print("Cannot determine emotion")

prediction_v = test_generator.labels[10,0]
prediction_a = test_generator.labels[10,1]

if prediction_v >= -0.5 and prediction_v <= 0.5 and prediction_a <= -0.66:
    print("focus")
elif prediction_v >= -0.2 and prediction_a >= -0.66 and prediction_a <= 0:
    print("relaxation")
elif prediction_v <= 0 and prediction_a <= 0:
    print("stress")
elif prediction_v >= 0 and prediction_v <= 0.4 and prediction_a >= 0 and prediction_a <= 0.4:
    print("interest")
elif (prediction_v >= 0 and prediction_v <= 0.7 and prediction_a >= 0.4 and prediction_a <= 0.7 or
      prediction_v >= 0.4 and prediction_v <= 0.7 and prediction_a >= 0 and prediction_a <= 0.4 or
      prediction_v >= 0.7 and prediction_v <= 1 and prediction_a >= 0 and prediction_a <= 1):
    print("engagement")
elif prediction_v >= 0 and prediction_v <= 1 and prediction_a >= 0.7 and prediction_a <= 1:
    print("excitement")
else:
    print("Cannot determine emotion")

val_rmse = np.sqrt(np.mean((prediction[:,0]-test_generator.labels[:,0])**2))
val_rmse

aur_rmse = np.sqrt(np.mean((prediction[:,1]-test_generator.labels[:,1])**2))
aur_rmse

def concordance_cc2(r1, r2):
     mean_cent_prod = ((r1 - r1.mean()) * (r2 - r2.mean())).mean()
     return (2 * mean_cent_prod) / (r1.var() + r2.var() + (r1.mean() - r2.mean()) ** 2)

Val_CCC = concordance_cc2(prediction[:,0], test_generator.labels[:,0])
Val_CCC

Aur_CCC = concordance_cc2(prediction[:,1], test_generator.labels[:,1])
Aur_CCC

with open(save_path+'/Val_CCC', 'wb') as file_pi:
        pickle.dump(Val_CCC, file_pi)

with open(save_path+'/Aur_CCC', 'wb') as file_pi:
        pickle.dump(Aur_CCC, file_pi)